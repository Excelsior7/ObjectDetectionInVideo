{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73cf13d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T10:30:52.581472Z",
     "start_time": "2022-12-02T10:30:17.420041Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "from transformers import DetrFeatureExtractor, DetrForObjectDetection\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14b71bf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T21:39:32.874026Z",
     "start_time": "2022-11-05T21:39:29.931741Z"
    }
   },
   "outputs": [],
   "source": [
    "class objectDetectorInVideo:\n",
    "    \n",
    "    # The different colors available for the predicted boxes. \n",
    "    colors = [(255,0,0),(0,255,0),(0,0,255),(255,255,0),(0,255,255),(255,0,255),(255,255,255)];\n",
    "    number_of_colors = len(colors);\n",
    "    \n",
    "    # Model for object detection.\n",
    "    # https://huggingface.co/facebook/detr-resnet-50.\n",
    "    feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50\");\n",
    "    model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\");\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"the objectDetectorInVideo instance correctly created\");\n",
    "    \n",
    "    \n",
    "    # Main interface between the user and the functionality of the objectDetectorInVideo.\n",
    "    # This functionality consists of:\n",
    "    # 1: Taking a video in input.\n",
    "    # 2: Identify objects of interest and draw boxes around them (if the model detects them).\n",
    "    # 3: Save the processed video in a new file.\n",
    "    #\n",
    "    # Note: The code creates a temporary file \"temp_file.mp4\" to store the processed video without audio.\n",
    "    #       Then, the function that handle the audio part takes this \"temp_file.mp4\", set the audio to it\n",
    "    #       and save it to the <out_video_path> defined by the user.\n",
    "    #       Given the conditions of my experiments, using the same memory area for the processed video file, \n",
    "    #       and the final video-audio file gave problems, hence the introduction of this temporary file.\n",
    "    #\n",
    "    # @param string <in_video_path> - The input video to process.\n",
    "    # @param string <out_video_path> - The new file that will contain the new processed video.\n",
    "    # @param float <threshold> - The level of confidence we want the model to have (real number between 0 and 1).\n",
    "    # @param list of string <objects_of_interest> - String representation of objects of interest to the user.\n",
    "    #\n",
    "    def objectDetectionInVideo(self, in_video_path, out_video_path, threshold, objects_of_interest):\n",
    "        \n",
    "        if threshold < 0 or threshold > 1:\n",
    "            raise ValueError(\"The threshold must be a number between 0 and 1\");\n",
    "            \n",
    "        if not os.path.exists(in_video_path):\n",
    "            raise ValueError(f\"The path of the video {in_video_path} is incorrect\");\n",
    "\n",
    "        video = cv.VideoCapture(in_video_path);\n",
    "        frames_width = int(video.get(3));\n",
    "        frames_height = int(video.get(4));\n",
    "\n",
    "        fourcc = cv.VideoWriter_fourcc(*'mp4v');\n",
    "        out = cv.VideoWriter(\"temp_file.mp4\", fourcc,video.get(cv.CAP_PROP_FPS), (frames_width,frames_height));\n",
    "        \n",
    "        objects_indices = self.__objectsPreprocessing(objects_of_interest);\n",
    "        \n",
    "        while video.isOpened():\n",
    "            is_frame_read_correctly, frame = video.read();\n",
    "\n",
    "            if not is_frame_read_correctly:\n",
    "                break;\n",
    "\n",
    "            input_frame = self.__framePreprocessing(frame);\n",
    "\n",
    "            x = self.feature_extractor(input_frame);\n",
    "            y = self.model(torch.tensor(x.pixel_values), torch.tensor(x.pixel_mask));\n",
    "\n",
    "            predicted_boxes = self.__processModelOutputs(y, frames_width, frames_height, threshold, objects_indices);\n",
    "            \n",
    "            # Draw predicted boxes (rectangle) around each object of interest identified.\n",
    "            # pt1 are the coordinates (x1,y1) of a corner of the rectangle.\n",
    "            # pt2 are the coordinates (x2,y2) of the corner of the rectangle at the opposite of pt1.\n",
    "            for i in range(len(predicted_boxes)):\n",
    "                pb = predicted_boxes[i];\n",
    "                pt1 = (int(pb[0]-pb[2]/2), int(pb[1]-pb[3]//2));\n",
    "                pt2 = (int(pb[0]+pb[2]//2), int(pb[1]+pb[3]//2));\n",
    "                cv.rectangle(frame, pt1, pt2, self.colors[i % self.number_of_colors], 3);\n",
    "                \n",
    "            out.write(frame);\n",
    "\n",
    "        video.release();\n",
    "        out.release();\n",
    "        \n",
    "        self.__setAudio(in_video_path, out_video_path);\n",
    "        os.remove(\"temp_file.mp4\");\n",
    "    \n",
    "    # Take the processed video (video with predicted boxes around each objects of interest detected),\n",
    "    # and set up its audio using the audio of the initial unprocessed video.\n",
    "    #\n",
    "    # @param string <in_video_path> - The input video to process.\n",
    "    # @param string <out_video_path> - The new file that will contain the new processed video.\n",
    "    #\n",
    "    def __setAudio(self, in_video_path, out_video_path):\n",
    "        videoclip_in = VideoFileClip(in_video_path);\n",
    "        audioclip_in = videoclip_in.audio;\n",
    "\n",
    "        videoclip_out = VideoFileClip(\"temp_file.mp4\");\n",
    "        videoclip_out_with_audio = videoclip_out.set_audio(audioclip_in);\n",
    "\n",
    "        videoclip_out_with_audio.write_videofile(out_video_path);\n",
    "    \n",
    "\n",
    "    # Convert the objects of interest to the user from string representation to index representation.\n",
    "    #\n",
    "    # @param list of string <objects_of_interest> - String representation of objects of interest to the user.\n",
    "    #\n",
    "    # @return list of int <object_indices> - Index representation of objects of interest to the user.\n",
    "    def __objectsPreprocessing(self, objects_of_interest):\n",
    "        objects_indices = [];\n",
    "        \n",
    "        try:\n",
    "            for obj in objects_of_interest:\n",
    "                objects_indices.append(self.model.config.label2id[obj]);\n",
    "        except KeyError:\n",
    "            print(f\"\"\"Object {obj} is not handled by the code, please refer \n",
    "                  to the list of handled objects https://cocodataset.org/#explore \"\"\");\n",
    "                \n",
    "        return list(set(objects_indices));\n",
    "                \n",
    "        \n",
    "    # Pre-processing step on the frame before entering the model.\n",
    "    #\n",
    "    # @param numpy.ndarray <frame> Unprocessed video frame.\n",
    "    #\n",
    "    # @return torch.tensor - Processed video frame.\n",
    "    def __framePreprocessing(self, frame):\n",
    "        return torch.from_numpy(frame).transpose(1,2).transpose(0,1);\n",
    "    \n",
    "    \n",
    "    # The goal here will be to find the predicted boxes of objects of interest in an image (if they appear in it),\n",
    "    # if the model detects the object with over <threshold> confidence.\n",
    "    # What determines whether an object is of interest or not is whether its index is in <object_indices>.\n",
    "    #\n",
    "    # @param detr.DetrObjectDetectionOutput <y> - Output of the model DetrForObjectDetection.\n",
    "    # @param int <width> - Width of the frames.\n",
    "    # @param int <height> - Height of the frames.\n",
    "    # @param float <threshold> - Level of confidence we want the model to have in its prediction.\n",
    "    # @param list of int <objects_indices> - Index representation of objects of interest to the user.\n",
    "    #\n",
    "    # @return list of float <predicted_boxes> The predicted boxes found composed of 4 elements each\n",
    "    #                                         (object x center, object y center, object width, object height).\n",
    "    def __processModelOutputs(self, y, width, height, threshold, objects_indices):\n",
    "        items = [];\n",
    "        predicted_boxes = [];\n",
    "\n",
    "        # The DETR model outputs 100 \"object queries\" (designation given by the authors) per image.\n",
    "        # Each object query is an attempt to find one object in the given image.\n",
    "        # More concretely each object query is a vector of probabilities of size num_classes+1,\n",
    "        # (+1 here refers to the N/A class). \n",
    "        # Each object query is in y.logits whose shape is (batch_size, num_queries, num_classes+1).\n",
    "        object_queries = torch.nn.functional.softmax(y.logits[0],dim=1);\n",
    "        predictions = torch.max(object_queries, dim=1);\n",
    "        \n",
    "        for index_object_query, (object_index,confidence) in enumerate(zip(predictions.indices, predictions.values)):\n",
    "            if object_index in objects_indices and confidence >= threshold:\n",
    "                items.append(index_object_query);\n",
    "        \n",
    "        # Each predicted box is composed of four elements:\n",
    "        # pb[0] = object x center, normalized between [0,1].\n",
    "        # pb[1] = object y center, normalized between [0,1].\n",
    "        # pb[2] = object width, normalized between [0,1].\n",
    "        # pb[3] = object height, normalized between [0,1].\n",
    "        for index_object_query in items:\n",
    "            pb = y.pred_boxes[0][index_object_query];\n",
    "            pb[0] = pb[0]*width;\n",
    "            pb[1] = pb[1]*height;\n",
    "\n",
    "            pb[2] = pb[2]*width;\n",
    "            pb[3] = pb[3]*height;\n",
    "\n",
    "            predicted_boxes.append(pb);\n",
    "\n",
    "        return predicted_boxes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "757fd3ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T21:39:37.665159Z",
     "start_time": "2022-11-05T21:39:37.660158Z"
    }
   },
   "outputs": [],
   "source": [
    "in_video_path = \"../data/miss_dior.mp4\";\n",
    "out_video_path = \"../data/miss_dior_output.mp4\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3e176a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T21:39:40.039100Z",
     "start_time": "2022-11-05T21:39:40.033599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the objectDetectorInVideo instance correctly created\n"
     ]
    }
   ],
   "source": [
    "obj = objectDetectorInVideo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e9da034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T21:39:57.067870Z",
     "start_time": "2022-11-05T21:39:57.063943Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj.objectDetectionInVideo(in_video_path, out_video_path, 0.9, [\"person\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.objectdetection_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31e5599adde77047070fe54788fa9ea706ddace5f754d0f722deac7b2ff9e06f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
